local lrCounter = 1
local logspaceLearningRates = torch.logspace(-2, -4, 200)
    if opt.lr_decay > 0 and counter % opt.lr_decay == 0 then
        local lr = logspaceLearningRates[lrCounter]
        lrCounter = lrCounter + 1
        print('Decreasing learning rate to ' .. lr)

        -- create new optimState to reset momentum
        optimState = {
          --learningRate = opt.lr,
          learningRate = logspaceLearningRates[lrCounter],
          beta1 = opt.beta1,
          weightDecay = 0.0005,
        }
      end


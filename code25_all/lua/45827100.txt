backward
model:updateParameters(<learning_rate>)
backward
updateParameters
optim.sgd
optim.adam
nn.StochasticGradient
updateParameters
updateParameters
require 'nn'
require 'optim'

local model = nn.Sequential()
model:add(nn.Linear(4, 1, false))
local params, grads = model:getParameters()

local criterion = nn.MSECriterion()
local inputs    = torch.randn(1, 4)
local labels    = torch.Tensor{1}

print(params)

model:zeroGradParameters()
local output = model:forward(inputs)
local loss   = criterion:forward(output, labels)
local dfdw   = criterion:backward(output, labels)
model:backward(inputs, dfdw)

-- With the line below uncommented, the parameters are updated:
-- model:updateParameters(1000)

print(params)


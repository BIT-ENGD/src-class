-- perceptron

require 'nn'

-- data
data = torch.Tensor({ {0, 0}, {0, 1}, {1, 0}, {1, 1} })
-- target
target = torch.Tensor({ 0, 1, 1, 0 })

-- model
perceptron = nn.Linear(2, 1)
-- loss function
criterion = nn.MSECriterion()

-- training
for i = 1, 10000 do
   -- set gradients to zero
   perceptron:zeroGradParameters()
   -- compute output
   output = perceptron:forward(data)
   -- compute loss
   loss = criterion:forward(output, target)
   -- compute gradients w.r.t. output
   dldo = criterion:backward(output, target)
   -- compute gradients w.r.t. parameters
   perceptron:backward(data,dldo)
   -- gradient descent with learningRate = 0.1
   perceptron:updateParameters(0.1)
   print(loss)
end

-- multilayer perceptron

require 'nn'

-- data
data = torch.Tensor({ {0, 0}, {0, 1}, {1, 0}, {1, 1} })
-- target
target = torch.Tensor({ 0, 1, 1, 0 })

-- model
multilayer = nn.Sequential()
inputs = 2; outputs = 1; HUs = 1;
multilayer:add(nn.Linear(inputs, HUs))
multilayer:add(nn.Tanh())
multilayer:add(nn.Linear(HUs, outputs))
-- loss function
criterion = nn.MSECriterion()

-- training
for i = 1, 10000 do
   -- set gradients to zero
   multilayer:zeroGradParameters()
   -- compute output
   output = multilayer:forward(data)
   -- compute loss
   loss = criterion:forward(output, target)
   -- compute gradients w.r.t. output
   dldo = criterion:backward(output, target)
   -- compute gradients w.r.t. parameters
   multilayer:backward(data,dldo)
   -- gradient descent with learningRate = 0.1
   multilayer:updateParameters(0.1)
   print(loss)
end


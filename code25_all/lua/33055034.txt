function mkPrimitive()
    local inp  = nn.Linear(2, 2)()
    local outp = nn.Tanh()(nn.Linear(2, 2)(nn.Tanh()(inp)))
    return nn.gModule({inp}, {outp})
end
prim = mkPrimitive()

toTrain
function mkNet()
    local fst = prim:clone('weight', 'gradWeight', 'bias', 'gradBias')()
    local snd = prim:clone('weight', 'gradWeight', 'bias', 'gradBias')(fst)
    return nn.gModule({fst}, {snd})
end

toTrain = mkNet()

prim
toTrain
prim
numRuns = 10
function train()
    local crit = nn.MSECriterion()
    for i = 1, numRuns do
        toTrain:zeroGradParameters()

        local inData = torch.rand(1, 2)  --make some input/output data
        local outData = torch.rand(1, 2)

        local pred = toTrain:forward(inData)
        local err = crit:forward(pred, outData)
        local grad = crit:backward(pred, outData)
        toTrain:backward(inData, grad)
        toTrain:updateParameters(0.01)

        local bigWs = toTrain:getParameters()
        local primWs = prim:getParameters()

        print(bigWs) --the params for the big network change during learning,
        print(primWs) --but the ones for the primitive don't.
        print("------------------------------")
    end
end
train()


#!/usr/bin/env python

import Queue
import time
import urllib2
import threading
import datetime

hosts = ["http://example.com/", "http://www.google.com"]

queue = Queue.Queue()

class ThreadUrl(threading.Thread):
    """Threaded Url Grab"""
    def __init__(self, queue):
            threading.Thread.__init__(self)
            self.queue = queue

    def run(self):
            while True:
                    #grabs host from queue
                    host = self.queue.get()

                    #grabs urls of hosts and prints first 1024 bytes of page
                    url = urllib2.urlopen(host)
                    print url.read(10)

                    #signals to queue job is done
                    self.queue.task_done()

start = time.time()

def main():

    #spawn a pool of threads, and pass them queue instance
    for i in range(1):
            t = ThreadUrl(queue)
            t.setDaemon(True)
            t.start()

            for host in hosts:
                    queue.put(host)

    queue.join()
main()
print "Elapsed time: %s" % (time.time() - start)


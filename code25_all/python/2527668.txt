import urllib2    
def get_source_html_proxy(url, pip, timeout):
# timeout in seconds (maximum number of seconds willing for the code to wait in
# case there is a proxy that is not working, then it gives up) 
    proxy_handler = urllib2.ProxyHandler({'http': pip})
    opener = urllib2.build_opener(proxy_handler)
    opener.addheaders = [('User-agent', 'Mozilla/5.0')]
    urllib2.install_opener(opener)
    req=urllib2.Request(url)
    sock=urllib2.urlopen(req)
    timp=0 # a counter that is going to measure the time until the result (webpage) is
           # returned
    while 1:
        data = sock.read(1024)
        timp=timp+1
        if len(data) < 1024: break
        timpLimita=50000000 * timeout
        if timp==timpLimita: # 5 millions is about 1 second
            break
    if timp==timpLimita:
        print IPul + ": Connection is working, but the webpage is fetched in more than 50 seconds. This proxy returns the following IP: " + str(data)
        return str(data)
    else:
        print "This proxy " + IPul + "= good proxy. " + "It returns the following IP: " + str(data)
        return str(data)
# Now, I call the function to test it for one single proxy (IP:port) that does not support user and password (a public high anonymity proxy)
#(I put a proxy that I know is working - slow, but is working)
rez=get_source_html_proxy("http://www.whatismyip.com/automation/n09230945.asp", "93.84.221.248:3128", 50)
print rez


% The Clever Algorithms Project: http://www.CleverAlgorithms.com
% (c) Copyright 2010 Jason Brownlee. Some Rights Reserved. 
% This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 2.5 Australia License.

% This is an algorithm description, see:
% Jason Brownlee. A Template for Standardized Algorithm Descriptions. Technical Report CA-TR-20100107-1, The Clever Algorithms Project http://www.CleverAlgorithms.com, January 2010.

% Name
% The algorithm name defines the canonical name used to refer to the technique, in addition to common aliases, abbreviations, and acronyms. The name is used in terms of the heading and sub-headings of an algorithm description.
\section{Immune Network Algorithm} 
\label{sec:immune_network_algorithm}
\index{Artificial Immune Network}
\index{aiNet}
\index{opt-aiNet}

% other names
% What is the canonical name and common aliases for a technique?
% What are the common abbreviations and acronyms for a technique?
\emph{Artificial Immune Network, aiNet, Optimization Artificial Immune Network, opt-aiNet.}

% Taxonomy: Lineage and locality
% The algorithm taxonomy defines where a techniques fits into the field, both the specific subfields of Computational Intelligence and Biologically Inspired Computation as well as the broader field of Artificial Intelligence. The taxonomy also provides a context for determining the relation- ships between algorithms. The taxonomy may be described in terms of a series of relationship statements or pictorially as a venn diagram or a graph with hierarchical structure.
\subsection{Taxonomy}
% To what fields of study does a technique belong?
The Artificial Immune Network algorithm (aiNet) is a Immune Network Algorithm from the field of Artificial Immune Systems.
% What are the closely related approaches to a technique?
It is related to other Artificial Immune System algorithms such as the Clonal Selection Algorithm (Section~\ref{sec:clonal_selection_algorithm}), the Negative Selection Algorithm (Section~\ref{sec:negative_selection_algorithm}), and the Dendritic Cell Algorithm (Section~\ref{sec:dca}).
% others
The Artificial Immune Network algorithm includes the base version and the extension for optimization problems called the Optimization Artificial Immune Network algorithm (opt-aiNet).

% Inspiration: Motivating system
% The inspiration describes the specific system or process that provoked the inception of the algorithm. The inspiring system may non-exclusively be natural, biological, physical, or social. The description of the inspiring system may include relevant domain specific theory, observation, nomenclature, and most important must include those salient attributes of the system that are somehow abstractly or conceptually manifest in the technique. The inspiration is described textually with citations and may include diagrams to highlight features and relationships within the inspiring system.
% Optional
\subsection{Inspiration}
% What is the system or process that motivated the development of a technique?
The Artificial Immune Network algorithm is inspired by the Immune Network theory of the acquired immune system.
% Which features of the motivating system are relevant to a technique?
The clonal selection theory of acquired immunity accounts for the adaptive behavior of the immune system including the ongoing selection and proliferation of cells that select-for potentially harmful (and typically foreign) material in the body.
A concern of the clonal selection theory is that it presumes that the repertoire of reactive cells remains idle when there are no pathogen to which to respond. Jerne proposed an Immune Network Theory (Idiotypic Networks) where immune cells are not at rest in the absence of pathogen, instead antibody and immune cells recognize and respond to each other \cite{Jerne1974, Jerne1974a, Jerne1984}. 

The Immune Network theory proposes that antibody (both free floating and surface bound) possess idiotopes (surface features) to which the receptors of other antibody can bind. As a result of receptor interactions, the repertoire becomes dynamic, where receptors continually both inhibit and excite each other in complex regulatory networks (chains of receptors). The theory suggests that the clonal selection process may be triggered by the idiotopes of other immune cells and molecules in addition to the surface characteristics of pathogen, and that the maturation process applies both to the receptors themselves and the idiotopes which they expose.

% Metaphor: Explanation via analogy
% The metaphor is a description of the technique in the context of the inspiring system or a different suitable system. The features of the technique are made apparent through an analogous description of the features of the inspiring system. The explanation through analogy is not expected to be literal scientific truth, rather the method is used as an allegorical communication tool. The inspiring system is not explicitly described, this is the role of the ‘inspiration’ element, which represents a loose dependency for this element. The explanation is textual and uses the nomenclature of the metaphorical system.
% Optional
\subsection{Metaphor}
% What is the explanation of a technique in the context of the inspiring system?
% What are the functionalities inferred for a technique from the analogous inspiring system?
The immune network theory has interesting resource maintenance and signaling information processing properties.
The classical clonal selection and negative selection paradigms integrate the accumulative and filtered learning of the acquired immune system, whereas the immune network theory proposes an additional order of complexity between the cells and molecules under selection. In addition to cells that interact directly with pathogen, there are cells that interact with those reactive cells and with pathogen indirectly, in successive layers such that networks of activity for higher-order structures such as internal images of pathogen (promotion), and regulatory networks (so-called anti-idiotopes and anti-anti-idiotopes).

% Strategy: Problem solving plan
% The strategy is an abstract description of the computational model. The strategy describes the information processing actions a technique shall take in order to achieve an objective. The strategy provides a logical separation between a computational realization (procedure) and a analogous system (metaphor). A given problem solving strategy may be realized as one of a number specific algorithms or problem solving systems. The strategy description is textual using information processing and algorithmic terminology.
\subsection{Strategy}
% What is the information processing objective of a technique?
The objective of the immune network process is to prepare a repertoire of discrete pattern detectors for a given problem domain, where better performing cells suppress low-affinity (similar) cells in the network.
% What is a techniques plan of action?
This principle is achieved through an interactive process of exposing the population to external information to which it responds with both a clonal selection response and internal meta-dynamics of intra-population responses that stabilizes the responses of the population to the external stimuli.

% Procedure: Abstract computation
% The algorithmic procedure summarizes the specifics of realizing a strategy as a systemized and parameterized computation. It outlines how the algorithm is organized in terms of the data structures and representations. The procedure may be described in terms of software engineering and computer science artifacts such as Pseudocode, design diagrams, and relevant mathematical equations.
\subsection{Procedure}
% What is the computational recipe for a technique?
% What are the data structures and representations used in a technique?
Algorithm~\ref{alg:opt_ainet} provides a pseudocode listing of the Optimization Artificial Immune Network algorithm (opt-aiNet) for minimizing a cost function. 

\begin{algorithm}[ht]
	\SetLine  

	% data
	\SetKwData{Best}{$S_{best}$}
	\SetKwData{NumClones}{$N_{clones}$}
	\SetKwData{NumRandomCells}{$N_{random}$}
	\SetKwData{Progeny}{Progeny}
	\SetKwData{Clones}{Clones}
	\SetKwData{ProblemSize}{ProblemSize}
	\SetKwData{Population}{Population}
	\SetKwData{PopulationSize}{$Population_{size}$}
	\SetKwData{Cell}{$Cell_{i}$}
	\SetKwData{Clone}{$Clone_{i}$}
	\SetKwData{Cost}{$Cost_{avg}$}
	\SetKwData{AffinityThreshold}{AffinityThreshold}
	% functions
	\SetKwFunction{InitializePopulation}{InitializePopulation}  
	\SetKwFunction{EvaluatePopulation}{EvaluatePopulation} 
	\SetKwFunction{GetBestSolution}{GetBestSolution} 
	\SetKwFunction{CreateRandomCells}{CreateRandomCells}
	\SetKwFunction{StopCondition}{StopCondition}
	\SetKwFunction{MutateRelativeToFitnessOfParent}{MutateRelativeToFitnessOfParent}
	\SetKwFunction{CalculateAveragePopulationCost}{CalculateAveragePopulationCost}
	\SetKwFunction{CreateClones}{CreateClones}  
	\SetKwFunction{SupressLowAffinityCells}{SupressLowAffinityCells}

	% I/O
	\KwIn{\PopulationSize, \ProblemSize, \NumClones, \NumRandomCells, \AffinityThreshold}		
	\KwOut{\Best}
  % Algorithm
	% initialize	
	\Population $\leftarrow$ \InitializePopulation{\PopulationSize, \ProblemSize}\;
	% loop
	\While{$\neg$\StopCondition{}} {
		% evaluate
		\EvaluatePopulation{\Population}\;
		% best
		\Best $\leftarrow$ \GetBestSolution{\Population}\;
		% state of population
		\Progeny $\leftarrow \emptyset$\;
		\Cost $\leftarrow$ \CalculateAveragePopulationCost{\Population}\;
		\While{\CalculateAveragePopulationCost{\Population} $>$ \Cost} {
			\ForEach{\Cell $\in$ \Population}{
				% clone
				\Clones $\leftarrow$ \CreateClones{\Cell, \NumClones}\;
				\ForEach{\Clone $\in$ \Clones} {
					% mutate
					\Clone $\leftarrow$ \MutateRelativeToFitnessOfParent{\Clone, \Cell}\;				
				}
				% select best from the clone
				\EvaluatePopulation{\Clones}\;
				\Progeny $\leftarrow$ \GetBestSolution{\Clones}\;			
			}
		}
		% network effects
		\SupressLowAffinityCells{\Progeny, \AffinityThreshold}\;
		% random cells
		\Progeny $\leftarrow$ \CreateRandomCells{\NumRandomCells}\;		
		% replace
		\Population $\leftarrow$ \Progeny\;
	}
	\Return{\Best}\;
	% end
	\caption{Pseudocode for opt-aiNet.}
	\label{alg:opt_ainet}
\end{algorithm}

% Heuristics: Usage guidelines
% The heuristics element describe the commonsense, best practice, and demonstrated rules for applying and configuring a parameterized algorithm. The heuristics relate to the technical details of the techniques procedure and data structures for general classes of application (neither specific implementations not specific problem instances). The heuristics are described textually, such as a series of guidelines in a bullet-point structure.
\subsection{Heuristics}
% What are the suggested configurations for a technique?
% What are the guidelines for the application of a technique to a problem instance?
\begin{itemize}
	\item aiNet is designed for unsupervised clustering, where as the opt-aiNet extension was designed for pattern recognition and optimization, specifically multi-modal function optimization.
	\item The amount of mutation of clones is proportionate to the affinity of the parent cell with the cost function (better fitness, lower mutation).
	\item The addition of random cells each iteration adds a random-restart like capability to the algorithms.
	\item Suppression based on cell similarity provides a mechanism for reducing redundancy.
	\item The population size is dynamic, and if it continues to grow it may be an indication of a problem with many local optima or that the affinity threshold may needs to be increased.
	\item Affinity proportionate mutation is performed using $c' = c + \alpha \times N(1,0)$ where $\alpha = \frac{1}{\beta} \times exp(-f)$, $N$ is a Guassian random number, and $f$ is the fitness of the parent cell, $\beta$ controls the decay of the function and can be set to 100. 
	\item The affinity threshold is problem and representation specific, for example a $AffinityThreshold$ may be set to an arbitrary value such as 0.1 on a continuous function domain, or calculated as a percentage of the size of the problem space.
	\item The number of random cells inserted may be 40\% of the population size. 
	\item The number of clones created for a cell may be small, such as 10.
\end{itemize}

% Code Listing
% The code description provides a minimal but functional version of the technique implemented with a programming language. The code description must be able to be typed into an appropriate computer, compiled or interpreted as need be, and provide a working execution of the technique. The technique implementation also includes a minimal problem instance to which it is applied, and both the problem and algorithm implementations are complete enough to demonstrate the techniques procedure. The description is presented as a programming source code listing.
\subsection{Code Listing}
% How is a technique implemented as an executable program?
% How is a technique applied to a concrete problem instance?
Listing~\ref{optainet} provides an example of the Optimization Artificial Immune Network (opt-aiNet) implemented in the Ruby Programming Language.
% problem
The demonstration problem is an instance of a continuous function optimization that seeks $\min f(x)$ where $f=\sum_{i=1}^n x_{i}^2$, $-5.0\leq x_i \leq 5.0$ and $n=2$. The optimal solution for this basin function is $(v_0,\ldots,v_{n-1})=0.0$.
% algorithm
The algorithm is an implementation based on the specification by de~Castro and Von Zuben \cite{Castro2002c}.

% the listing
\lstinputlisting[firstline=7,language=ruby,caption=Optimization Artificial Immune Network in Ruby, label=optainet]{../src/algorithms/immune/optainet.rb}

% References: Deeper understanding
% The references element description includes a listing of both primary sources of information about the technique as well as useful introductory sources for novices to gain a deeper understanding of the theory and application of the technique. The description consists of hand-selected reference material including books, peer reviewed conference papers, journal articles, and potentially websites. A bullet-pointed structure is suggested.
\subsection{References}
% What are the primary sources for a technique?
% What are the suggested reference sources for learning more about a technique?

% 
% Primary Sources
% 
\subsubsection{Primary Sources}
% early
Early works, such as Farmer et~al. \cite{Farmer1986} suggested at the exploitation of the information processing properties of network theory for machine learning.
% pre-cursor
A seminal network theory based algorithm was proposed by Timmis et~al. for clustering problems called the Artificial Immune Network (AIN) \cite{Timmis2000} that was later extended and renamed the Resource Limited Artificial Immune System \cite{Timmis2001} and Artificial Immune Network (AINE) \cite{Knight2001}.
% seminal
The Artificial Immune Network (aiNet) algorithm was proposed by de~Castro and Von Zuben that extended the principles of the Artificial Immune Network (AIN) and the Clonal Selection Algorithm (CLONALG) and was applied to clustering \cite{Castro2000a}. The aiNet algorithm was further extended to optimization domains and renamed opt-aiNet \cite{Castro2002c}.

% 
% Learn More
% 
\subsubsection{Learn More}
% extensions
The authors de~Castro and Von Zuben provide a detailed presentation of the aiNet algorithm as a book chapter that includes immunological theory, a description of the algorithm, and demonstration application to clustering problem instances \cite{Castro2001}.
% fixes
Timmis and Edmonds provide a careful examination of the opt-aiNet algorithm and propose some modifications and augmentations to improve its applicability and performance for multimodal function optimization problem domains \cite{Timmis2004}.
% dynamic
The authors de~Franca, Von~Zuben, and de~Castro proposed an extension to opt-aiNet that provided a number of enhancements and adapted its capability for for dynamic function optimization problems called dopt-aiNet \cite{Franca2005}.



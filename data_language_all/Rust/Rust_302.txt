//! Provides various helpful layers, which might be not directly related to
//! neural networks in general.
//!
//! These layers do not have to necesarrely manipulate the data flowing through
//! them and might have
//! no effect on the Networks' capabilities to learn (e.g. loging) but obey all
//! the rules of a [Layer][1].
//! The type of these layers can vary a lot. From data normalization to
//! specific data access layers for e.g. a database like LevelDB.
//!
//! [1]: ../../layer/index.html
pub use self::flatten::Flatten;
pub use self::reshape::{Reshape, ReshapeConfig};

pub mod flatten;
pub mod reshape;
//! Provides methods to calculate the loss (cost) of some output.
//!
//! A loss function is also sometimes called cost function.
#[macro_export]
macro_rules! impl_ilayer_loss {
    () => (
        fn exact_num_output_blobs(&self) -> Option<usize> { Some(1) }
        fn exact_num_input_blobs(&self) -> Option<usize> { Some(1) }
        fn auto_output_blobs(&self) -> bool { true }

        fn loss_weight(&self, output_id: usize) -> Option<f32> {
            if output_id == 0 {
                Some(1f32)
            } else {
                None
            }
        }
    )
}

pub use self::negative_log_likelihood::{NegativeLogLikelihood, NegativeLogLikelihoodConfig};

pub mod negative_log_likelihood;

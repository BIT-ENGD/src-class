
layout: post
title: Парсинг блогов
category: projects

Проект о том, как скопировать блог.
Оглавление: 
- Постановка задачи
- Схема работы программы
- Парсинг
- Экспорт в Google Docs
- Результаты
Постановка задачи
Проблема: пользователь ведет блог на платформе, которая вот-вот закроется. Терять написанное годами он не хочет. Хочет сохранить для истории все, что писал, с картинками.
Задача: скопировать содержимое блога с блог-платформы в контролируемое пользователем место. 
Используемые технологии: Python, BeautifulSoup, Google Docs API
Схема работы программы

Парсинг

На вход программе подается адрес блога, с помощью request получаем первую страницу блога.
Программа парсит страницу блога. Она обычно состоит из нескольких постов. Пост состоит из элементов: заголовок, контент, дата, время, кол-во комментариев, url-поста. Значение каждого элемента записываются в словарь "Пост", этот словарь затем добавляется в качестве элемента словаря "Блог"
После парсинга всей страницы, ищется ссылка на следующую страницу, и следующая страница загружается.  
Предыдущие два пункта повторяеем, пока страницы не закончатся.
Словарь "Блог" записывается в JSON

Экспорт в Google Docs
У меня было несколько вариантов, где сохранять содержимое блога: MS Word, Google Docs, Блокнот или оставить сырой JSON. Варинт с вордом хороший, но недоступный из онлайна, а в 2021 - это важно. Можно, конечно, выгрузить его в Google Docs, но зачем лишние движения, если можно сразу. В блокнот не вставить картинки, а JSON не поймут блондинки. 

Создаем по гайду гугла заготовку для работы с Google Docs (аналог ворда) - пустой документ.
Пробегаемся по JSON, и в каждом элементе "Пост", записываем в переменные значение элементов эже самого "Поста". 
Анализируем содержимое элемента "Контент". В нем может быть форматирование текста, картинки, ссылки.
С помощью Google Docs API вставляем в документ каждый элемент и одновременно задаем ему форматирование его абзазу. 
Повторяем предыдущие два пункта для всех элементов "Пост"
У Google API если лимиты на использование. После нескольких десяток вставок нужно запустить таймер и ждать заданный таймаут. 

Результаты
Программа работает не быстро из-за таймаутов гугла, но поставленная задача решена.
Часть сформированного JSON случайного блога:
...
    "post220266050": {
        "header": "Заметка.",
        "content": ["Тот ещё сумрачный лес, конечно. ", "<br/>", "<br/>", "Не выходит с маяком, сооруди хотя бы кормушку  для птиц."],
        "date": "Воскресенье, 20 декабря 2020",
        "time": "14:50",
        "commentcount": "2",
        "url": "...p220266050_zametka.htm"
    },
    "post220162636": {
        "header": "Заметка.",
        "content": ["Отчасти «Сто лет чаепития», это спиритический сеанс."],
        "date": "Суббота, 21 ноября 2020",
        "time": "10:44",
        "commentcount": 0,
        "url": "...p220162636_zametka.htm"
    },
    "post219657688": {
        "header": "Столетнее чаепитие",
        "content": ["Некоторые разговоры происходят вне времени и географии. ", "<br/>", "<br/>", "Первая часть серии о параллельной коммуникации.", "<br/>", "<img src=\"...1/0/3/1/103167/thumb/86491019.jpg\"/>"],
        "date": "Четверг, 09 июля 2020",
        "time": "11:12",
        "commentcount": "6",
        "url": ...p219657688_stoletnee-chaepitie.htm"
    }
}
Результат экспорта в Google Docs:

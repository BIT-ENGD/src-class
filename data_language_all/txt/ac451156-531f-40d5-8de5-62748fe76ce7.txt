Old references

E. Bair, T. Hastie, D. Paul, and R. Tibshirani.
Prediction by supervised principal components. Journal of the American Statistical Association, 101(473)2006.
D. Paul, E. Bair, T. Hastie, and R. Tibshirani.
`Preconditioning' for feature selection and regression in high-dimensional problems. The Annals of Statistics, 36(4):1595--1618, 2008.
V. Q. Vu and J. Lei.
Minimax Sparse Principal Subspace Estimation in High Dimensions. Annals of Statistics, 41:2905--2947, 2013.
D. Homrighausen and D. J. McDonald.
On the Nyström and Column-Sampling Methods for the Approximate Principal Components Analysis of Large Data Sets. Journal of Computational and Graphical Statistics, 25(2):344--362, 2016.
L. Ding and D. J. McDonald.
Predicting phenotypes from microarrays using amplified, initially marginal, eigenvector regression. 2017.
R. D. Cook, L. Forzani, and others.
Principal fitted components for dimension reduction in regression. Statistical Science, 23(4):485--501, 2008.
L. Li and X. Yin.
Sliced inverse regression with regularizations. Biometrics, 64(1):124--131, 2008.
T. T. Cai and A. Zhang.
Rate-optimal perturbation bounds for singular subspaces with applications to high-dimensional statistics. 2016.
K. P. Adragni and R. D. Cook.
Sufficient dimension reduction and prediction in regression. Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, 367(1906):4385--4405, 2009.
X. Chen, C. Zou, and R. D. Cook.
Coordinate-independent sparse sufficient dimension reduction and variable selection. The Annals of Statistics, 38(6):3696--3723, 2010.
L. Li.
Sparse sufficient dimension reduction. Biometrika, 94(3):603--613, 2007.
I. M. Johnstone and A. Y. Lu.
Sparse principal components analysis. 2009.
B. Li and S. Wang.
On directional regression for dimension reduction. Journal of the American Statistical Association, 102(479):997--1008, 2007.
A. Globerson and N. Tishby.
Sufficient dimensionality reduction. Journal of Machine Learning Research, 3:1307--1331, 2003.
N. Arcolano and P. J. Wolfe.
Estimating principal components of covariance matrices using the Nystr\"om method. 2011.
N. Arcolano and P. J. Wolfe.
Estimating principal components of large covariance matrices using the Nyström method. Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, 3784--3787, 2011.
Y. Yu, T. Wang, and R. J. Samworth.
A useful variant of the Davis--Kahan theorem for statisticians. Biometrika, 102(2):315--323, 2014.
J. Lei, V. Q. Vu, and others.
Sparsistency and agnostic inference in sparse PCA. The Annals of Statistics, 43(1):299--322, 2015.
T. Wang, Q. Berthet, R. J. Samworth, and others.
Statistical and computational trade-offs in estimation of sparse principal components. The Annals of Statistics, 44(5):1896--1930, 2016.
A. Kneip and K. J. Utikal.
Inference for density families using functional principal component analysis. Journal of the American Statistical Association, 96(454):519--542, 2001.
W. Yang and H. Xu.
Streaming Sparse Principal Component Analysis. Proceedings of the 32nd International Conference on Machine Learning, Proceedings of Machine Learning Research, 37, 494--503, 2015.
V. Q. Vu, J. Cho, J. Lei, and K. Rohe.
Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA. Advances in Neural Information Processing Systems 26, 2670--2678, 2013.

New references (10/27/2017)
Some duplicates

V. Q. Vu, J. Cho, J. Lei, and K. Rohe.
Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA. Advances in Neural Information Processing Systems 26, 2670--2678, 2013.
J. Wang, J. Lee, M. Mahdavi, M. Kolar, and N. Srebro.
Sketching Meets Random Projection in the Dual: A Provable Recovery Algorithm for Big and High-dimensional Data. Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), Proceedings of Machine Learning Research, 54, 1150--1158, 2017.
L. Zhang, M. Mahdavi, R. Jin, T. Yang, and S. Zhu.
Random projections for classification: A recovery approach. IEEE Transactions on Information Theory, 60(11):7300--7316, 2014.
L. Zhang, M. Mahdavi, R. Jin, T. Yang, and S. Zhu.
Recovering the Optimal Solution by Dual Random Projection. Proceedings of the 26th Annual Conference on Learning Theory, Proceedings of Machine Learning Research, 30, 135--157, 2013.
Q. Li and J. Shao.
Sparse quadratic discriminant analysis for high dimensional data. Statistica Sinica, 25:457--473, 2015.
J. A. Ramey, C. K. Stein, P. D. Young, and D. M. Young.
High-Dimensional Regularized Discriminant Analysis. 2016.
J. Fan, Y. Liao, and M. Mincheva.
Large covariance estimation by thresholding principal orthogonal complements. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 75(4):603--680, 2013.
T. I. Cannings and R. J. Samworth.
Random-projection ensemble classification. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(4):959--1035, 2017.
J. Fan, Y. Liao, and W. Wang.
Projected principal component analysis in factor models. Annals of Statistics, 44(1):219--254, 2016.
Y. Fan, J. Jin, and Z. Yao.
Optimal classification in sparse Gaussian graphic model. Annals of Statistics, 41(5):2537--2571, 2013.
Q. Mai and H. Zou.
A Note On the Connection and Equivalence of Three Sparse Linear Discriminant Analysis Methods. Technometrics, 55(2):243-246, 2013.
J. Fan, Z. T. Ke, H. Liu, and L. Xia.
QUADRO: A supervised dimension reduction method via Rayleigh quotient optimization. Annals of Statistics, 43(4):1498--1534, 2015.
